{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages, which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission if necessary. \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission. \n",
    "\n",
    "In addition to implementing code, there is a writeup to complete. The writeup should be completed in a separate file, which can be either a markdown file or a pdf document. There is a [write up template](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup_template.md) that can be used to guide the writing process. Completing the code template and writeup template will cover all of the [rubric points](https://review.udacity.com/#!/rubrics/481/view) for this project.\n",
    "\n",
    "The [rubric](https://review.udacity.com/#!/rubrics/481/view) contains \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. The stand out suggestions are optional. If you decide to pursue the \"stand out suggestions\", you can include the code in this Ipython notebook and also discuss the results in the writeup file.\n",
    "\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T09:28:36.578517",
     "start_time": "2017-02-07T09:28:36.242916"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"train.p\"\n",
    "testing_file = \"test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "X_train_load, y_train_load = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n",
    "Complete the basic data summary below. Use python, numpy and/or pandas methods to calculate the data summary rather than hard coding the results. For example, the [pandas shape method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shape.html) might be useful for calculating some of the summary results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T09:28:39.820787",
     "start_time": "2017-02-07T09:28:39.810451"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv('signnames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T09:29:12.801794",
     "start_time": "2017-02-07T09:29:12.789528"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 39209\n",
      "Number of testing examples = 7842\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = len(X_train_load)\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = len(X_test)\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = X_train_load[0].shape\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(set(y_train_load))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Include an exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc.\n",
    "\n",
    "The [Matplotlib](http://matplotlib.org/) [examples](http://matplotlib.org/examples/index.html) and [gallery](http://matplotlib.org/gallery.html) pages are a great resource for doing visualizations in Python.\n",
    "\n",
    "**NOTE:** It's recommended you start with something simple first. If you wish to do more, come back to it after you've completed the rest of the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T09:36:03.099878",
     "start_time": "2017-02-07T09:36:02.993027"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "General caution\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI+wS0RF2iegIu0T8P+kA2dQH9U5YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa25a82db00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "% matplotlib inline\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "index = random.randint(0, len(X_train_load))\n",
    "image = X_train_load[index].squeeze()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(1, 1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "print(y_train_load[index])\n",
    "print(df.ix[y_train_load[index], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, len(X_train_load))\n",
    "image = X_train_load[index].squeeze()\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 43 artists>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEhSpwwASeqUASBJnfp/ycLiJrklVtQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b33018518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#plt.plot(y_train_load)\n",
    "\n",
    "bincount = np.bincount(y_train_load)\n",
    "xlabels = np.arange(len(bincount))\n",
    "plt.bar(xlabels, bincount,width=0.35,color='#d62728' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Neural network architecture\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these.\n",
    "\n",
    "**NOTE:** The LeNet-5 implementation shown in the [classroom](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) at the end of the CNN lesson is a solid starting point. You'll have to change the number of classes and possibly the preprocessing, but aside from that it's plug and play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pre-process the Data Set (normalization, grayscale, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Preprocess the data here. Preprocessing steps could include normalization, converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Split Data into Training, Validation and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T09:41:46.428899",
     "start_time": "2017-02-07T09:41:46.295623"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training set = 23525\n",
      "Number of validation set = 7842\n",
      "Number of test set = 7842\n"
     ]
    }
   ],
   "source": [
    "### Split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X_train_load, y_train_load, test_size=0.4)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_rest, y_rest, test_size=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "print(\"Number of training set =\", len(X_train))\n",
    "print(\"Number of validation set =\", len(X_validation))\n",
    "print(\"Number of test set =\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T10:47:00.198078",
     "start_time": "2017-02-07T10:47:00.194859"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T10:47:01.601670",
     "start_time": "2017-02-07T10:47:01.547373"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T10:47:02.598650",
     "start_time": "2017-02-07T10:47:02.586979"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the test set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T10:47:04.639275",
     "start_time": "2017-02-07T10:47:04.636252"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Calculate and report the accuracy on the training and validation set.\n",
    "### Once a final model architecture is selected, \n",
    "### the accuracy on the test set should be calculated and reported as well.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T10:47:05.745905",
     "start_time": "2017-02-07T10:47:05.321237"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T10:47:06.487625",
     "start_time": "2017-02-07T10:47:05.854459"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-07T16:10:17.953536",
     "start_time": "2017-02-07T16:06:31.589689"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.561\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.775\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.875\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.906\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.918\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.940\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.940\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.943\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.962\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here and use the model to output the prediction for each image.\n",
    "### Make sure to pre-process the images with the same pre-processing pipeline used earlier.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Calculate the accuracy for these 5 new images. \n",
    "### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For each of the new images, print out the model's softmax probabilities to show the **certainty** of the model's predictions (limit the output to the top 5 probabilities for each image). [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k) could prove helpful here. \n",
    "\n",
    "The example below demonstrates how tf.nn.top_k can be used to find the top k predictions for each image.\n",
    "\n",
    "`tf.nn.top_k` will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids.\n",
    "\n",
    "Take this numpy array as an example. The values in the array represent predictions. The array contains softmax probabilities for five candidate images with six possible classes. `tk.nn.top_k` is used to choose the three classes with the highest probability:\n",
    "\n",
    "```\n",
    "# (5, 6) array\n",
    "a = np.array([[ 0.24879643,  0.07032244,  0.12641572,  0.34763842,  0.07893497,\n",
    "         0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.08594638,  0.0178669 ,  0.18063401,\n",
    "         0.15899337],\n",
    "       [ 0.26076848,  0.23664738,  0.08020603,  0.07001922,  0.1134371 ,\n",
    "         0.23892179],\n",
    "       [ 0.11943333,  0.29198961,  0.02605103,  0.26234032,  0.1351348 ,\n",
    "         0.16505091],\n",
    "       [ 0.09561176,  0.34396535,  0.0643941 ,  0.16240774,  0.24206137,\n",
    "         0.09155967]])\n",
    "```\n",
    "\n",
    "Running it through `sess.run(tf.nn.top_k(tf.constant(a), k=3))` produces:\n",
    "\n",
    "```\n",
    "TopKV2(values=array([[ 0.34763842,  0.24879643,  0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.18063401],\n",
    "       [ 0.26076848,  0.23892179,  0.23664738],\n",
    "       [ 0.29198961,  0.26234032,  0.16505091],\n",
    "       [ 0.34396535,  0.24206137,  0.16240774]]), indices=array([[3, 0, 5],\n",
    "       [0, 1, 4],\n",
    "       [0, 5, 1],\n",
    "       [1, 3, 5],\n",
    "       [1, 4, 3]], dtype=int32))\n",
    "```\n",
    "\n",
    "Looking just at the first row we get `[ 0.34763842,  0.24879643,  0.12789202]`, you can confirm these are the 3 largest probabilities in `a`. You'll also notice `[3, 0, 5]` are the corresponding indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting traffic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile traffic.py\n",
    "# %load traffic.py\n",
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"train.p\"\n",
    "testing_file = \"test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "X_train_load, y_train_load = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(X_train_load.shape)\n",
    "print(y_train_load.shape)\n",
    "\n",
    "#X_train_load[:,:,:,:] = (X_train_load[:,:,:,:] - 128) \n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def RGB2YUV(dataset):\n",
    "    s1,s2,s3,s4 = dataset.shape\n",
    "    for i in range(dataset.shape[0]):\n",
    "        dataset[i,:,:,:]=cv2.cvtColor(dataset[i,:,:,:],cv2.COLOR_RGB2YUV)\n",
    "    dataset = np.reshape(dataset[:,:,:,0],(s1,s2,s3,1))\n",
    "    return dataset\n",
    "\n",
    "def RGB2GRY(dataset):\n",
    "    s1,s2,s3,s4 = dataset.shape\n",
    "    for i in range(dataset.shape[0]):\n",
    "        dataset[i,:,:,0]=cv2.cvtColor(dataset[i,:,:,:],cv2.COLOR_RGB2GRAY)\n",
    "    dataset = np.reshape(dataset[:,:,:,0],(s1,s2,s3,1))\n",
    "    return dataset\n",
    "\n",
    "X_train_load = RGB2YUV(X_train_load)\n",
    "\n",
    "\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X_train_load, y_train_load, test_size=0.4)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_rest, y_rest, test_size=0.5)\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "print(\"Number of training set =\", len(X_train))\n",
    "print(\"Number of validation set =\", len(X_validation))\n",
    "print(\"Number of test set =\", len(X_test))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "\n",
    "\n",
    "def variable_summaries(var,scope_name=\"summaries\"):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope(scope_name):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "\n",
    "def LeNet(x,keep_prob, fcp=(120,120,84)):\n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "\n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n",
    "\n",
    "    variable_summaries(x,\"x\")\n",
    "    with tf.name_scope(\"CONV1\"):\n",
    "        conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean=mu, stddev=sigma))\n",
    "        conv1_b = tf.Variable(tf.zeros(6))\n",
    "        conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    variable_summaries(conv1,\"conv1\")\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    with tf.name_scope(\"RELU1\"):\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "    variable_summaries(conv1,\"conv1_relu1\")\n",
    "\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    with tf.name_scope(\"POOL1\"):\n",
    "        conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    variable_summaries(conv1,\"pool1\")\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    with tf.name_scope(\"CONV2\"):\n",
    "        conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n",
    "        conv2_b = tf.Variable(tf.zeros(16))\n",
    "        conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    with tf.name_scope(\"RELU2\"):\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    with tf.name_scope(\"POOL2\"):\n",
    "        # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "        conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    with tf.name_scope(\"Flatten\"):\n",
    "        # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "        fc0 = flatten(conv2)\n",
    "\n",
    "    with tf.name_scope(\"FC1\"):\n",
    "        # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "        fc1_W = tf.Variable(tf.truncated_normal(shape=(400, fcp[0]), mean=mu, stddev=sigma))\n",
    "        fc1_b = tf.Variable(tf.zeros(fcp[0]))\n",
    "        fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "\n",
    "    with tf.name_scope(\"RELU3\"):\n",
    "        # SOLUTION: Activation.\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    with tf.name_scope(\"DROP_OUT\"):\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob=keep_prob)\n",
    "\n",
    "    with tf.name_scope(\"FC2\"):\n",
    "        # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        fc2_W = tf.Variable(tf.truncated_normal(shape=(fcp[0], fcp[1]), mean=mu, stddev=sigma))\n",
    "        fc2_b = tf.Variable(tf.zeros(fcp[1]))\n",
    "        fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "\n",
    "    with tf.name_scope(\"RELU4\"):\n",
    "        # SOLUTION: Activation.\n",
    "        fc2 = tf.nn.relu(fc2)\n",
    "        \n",
    "        \n",
    "    with tf.name_scope(\"FC2\"):\n",
    "        # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        fc3_W = tf.Variable(tf.truncated_normal(shape=(fcp[1], fcp[2]), mean=mu, stddev=sigma))\n",
    "        fc3_b = tf.Variable(tf.zeros(fcp[2]))\n",
    "        fc3 = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "\n",
    "    with tf.name_scope(\"RELU5\"):\n",
    "        # SOLUTION: Activation.\n",
    "        fc3 = tf.nn.relu(fc3)\n",
    "        \n",
    "\n",
    "    with tf.name_scope(\"FC4\"):\n",
    "        # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "        fc4_W = tf.Variable(tf.truncated_normal(shape=(fcp[2], 43), mean=mu, stddev=sigma))\n",
    "        fc4_b = tf.Variable(tf.zeros(43))\n",
    "        logits = tf.matmul(fc3, fc4_W) + fc4_b\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def LeNet_o(x,keep_prob, fcp=(120,120,84)):\n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "\n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n",
    "\n",
    "    variable_summaries(x,\"x\")\n",
    "    with tf.name_scope(\"CONV1\"):\n",
    "        conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean=mu, stddev=sigma))\n",
    "        conv1_b = tf.Variable(tf.zeros(6))\n",
    "        conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    variable_summaries(conv1,\"conv1\")\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    with tf.name_scope(\"RELU1\"):\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "    variable_summaries(conv1,\"conv1_relu1\")\n",
    "\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    with tf.name_scope(\"POOL1\"):\n",
    "        conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    variable_summaries(conv1,\"pool1\")\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    with tf.name_scope(\"CONV2\"):\n",
    "        conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n",
    "        conv2_b = tf.Variable(tf.zeros(16))\n",
    "        conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    with tf.name_scope(\"RELU2\"):\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    with tf.name_scope(\"POOL2\"):\n",
    "        # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "        conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    with tf.name_scope(\"Flatten\"):\n",
    "        # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "        fc0 = flatten(conv2)\n",
    "\n",
    "    with tf.name_scope(\"FC1\"):\n",
    "        # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "        fc1_W = tf.Variable(tf.truncated_normal(shape=(400, fcp[0]), mean=mu, stddev=sigma))\n",
    "        fc1_b = tf.Variable(tf.zeros(fcp[0]))\n",
    "        fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "\n",
    "    with tf.name_scope(\"RELU3\"):\n",
    "        # SOLUTION: Activation.\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    with tf.name_scope(\"DROP_OUT\"):\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob=keep_prob)\n",
    "\n",
    "    with tf.name_scope(\"FC2\"):\n",
    "        # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        fc2_W = tf.Variable(tf.truncated_normal(shape=(fcp[0], fcp[1]), mean=mu, stddev=sigma))\n",
    "        fc2_b = tf.Variable(tf.zeros(fcp[1]))\n",
    "        fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "\n",
    "    with tf.name_scope(\"RELU4\"):\n",
    "        # SOLUTION: Activation.\n",
    "        fc2 = tf.nn.relu(fc2)\n",
    "        \n",
    "    with tf.name_scope(\"FC2\"):\n",
    "        # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        fc3_W = tf.Variable(tf.truncated_normal(shape=(fcp[1], 43), mean=mu, stddev=sigma))\n",
    "        fc3_b = tf.Variable(tf.zeros(43))\n",
    "        logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "\n",
    "\n",
    "    return logits\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32, ())\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 160\n",
    "BATCH_SIZE = 1024\n",
    "rate = 0.001\n",
    "training_keep_prob = 0.5\n",
    "\n",
    "netpara = (300,180,100)\n",
    "#netpara = (120, 84)\n",
    "\n",
    "logits = LeNet(x,keep_prob, netpara)\n",
    "#logits = LeNet_o(x,keep_prob, netpara)\n",
    "\n",
    "tf.summary.image(\"x\", x,max_outputs=3)\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy_operation)\n",
    "#tf.summary.scalar(\"validation_accuracy\", validation_accuracy)\n",
    "\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob:1})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # sess.graph_def is the graph definition; that enables the Graph Visualizer.\n",
    "    file_writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob:training_keep_prob})\n",
    "\n",
    "            if 0: #offset % (BATCH_SIZE*100) == 0.0:\n",
    "                summary, Traing_loss = sess.run([merged,loss_operation], feed_dict={x: batch_x, y: batch_y, keep_prob:1})\n",
    "                print(\"batch = {:.3f}  Traing Loss = {:.3f}\".format(offset, Traing_loss))\n",
    "\n",
    "                file_writer.add_summary(summary, i*num_examples+offset)\n",
    "\n",
    "\n",
    "        Training_accuracy = evaluate(X_train, y_train)\n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ... Training Accuracy= {:.3f}  Validation Accuracy = {:.3f}\".format(i+1, Training_accuracy,validation_accuracy))\n",
    "\n",
    "        #file_writer.add_summary(summary,i)\n",
    "\n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "New Test file in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 32, 32, 3)\n",
      "(39209,)\n",
      "Number of training set = 23525\n",
      "Number of validation set = 7842\n",
      "Number of test set = 7842\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"train.p\"\n",
    "testing_file = \"test.p\"\n",
    "\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "\n",
    "X_train_load, y_train_load = train['features'], train['labels']\n",
    "X_test_load, y_test_load = test['features'], test['labels']\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(X_train_load.shape)\n",
    "print(y_train_load.shape)\n",
    "\n",
    "#X_train_load[:,:,:,:] = (X_train_load[:,:,:,:] - 128) \n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def RGB2YUV(dataset):\n",
    "    s1,s2,s3,s4 = dataset.shape\n",
    "    for i in range(dataset.shape[0]):\n",
    "        dataset[i,:,:,:]=cv2.cvtColor(dataset[i,:,:,:],cv2.COLOR_RGB2YUV)\n",
    "    dataset = np.reshape(dataset[:,:,:,0],(s1,s2,s3,1))\n",
    "    return dataset\n",
    "\n",
    "def RGB2GRY(dataset):\n",
    "    s1,s2,s3,s4 = dataset.shape\n",
    "    for i in range(dataset.shape[0]):\n",
    "        dataset[i,:,:,0]=cv2.cvtColor(dataset[i,:,:,:],cv2.COLOR_RGB2GRAY)\n",
    "    dataset = np.reshape(dataset[:,:,:,0],(s1,s2,s3,1))\n",
    "    return dataset\n",
    "\n",
    "X_train_load = RGB2YUV(X_train_load)\n",
    "\n",
    "\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X_train_load, y_train_load, test_size=0.4)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_rest, y_rest, test_size=0.5)\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "print(\"Number of training set =\", len(X_train))\n",
    "print(\"Number of validation set =\", len(X_validation))\n",
    "print(\"Number of test set =\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_9 (Convolution2D)  (None, 28, 28, 6)     156         convolution2d_input_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 28, 28, 6)     0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 14, 14, 6)     0           activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 10, 10, 16)    2416        maxpooling2d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 10, 10, 16)    0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 5, 5, 16)      0           activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 400)           0           maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 120)           48120       flatten_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 120)           0           dense_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 84)            10164       activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 84)            0           dense_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 43)            3655        activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 43)            0           dense_16[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 64,511\n",
      "Trainable params: 64,511\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: expected activation_26 to have shape (None, 43) but got array with shape (23525, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f126af0cb9cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m#model.train_on_batch(X_batch, Y_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hsu/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/hsu/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hsu/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1031\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m                                    \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                                    exception_prefix='model target')\n\u001b[0m\u001b[1;32m   1034\u001b[0m         sample_weights = standardize_sample_weights(sample_weight,\n\u001b[1;32m   1035\u001b[0m                                                     self.output_names)\n",
      "\u001b[0;32m/home/hsu/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: expected activation_26 to have shape (None, 43) but got array with shape (23525, 1)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Conv2D,Flatten,Convolution2D,MaxPooling2D\n",
    "\n",
    "\n",
    "model  = Sequential()\n",
    "\n",
    "model.add(Convolution2D(6, 5, 5, border_mode='valid', input_shape=(32, 32, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), border_mode='valid'))\n",
    "\n",
    "model.add(Convolution2D(16, 5, 5, border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), border_mode='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(output_dim=120))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(output_dim=84))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "\n",
    "model.add(Dense(output_dim=43))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, nb_epoch=5, batch_size=128)\n",
    "#model.train_on_batch(X_batch, Y_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "classes = model.predict_classes(X_test, batch_size=32)\n",
    "proba = model.predict_proba(X_test, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ... Training Accuracy= 0.108  Validation Accuracy = 0.108\n",
      "EPOCH 2 ... Training Accuracy= 0.173  Validation Accuracy = 0.170\n",
      "EPOCH 3 ... Training Accuracy= 0.265  Validation Accuracy = 0.257\n",
      "EPOCH 4 ... Training Accuracy= 0.372  Validation Accuracy = 0.358\n",
      "EPOCH 5 ... Training Accuracy= 0.497  Validation Accuracy = 0.480\n",
      "EPOCH 6 ... Training Accuracy= 0.620  Validation Accuracy = 0.609\n",
      "EPOCH 7 ... Training Accuracy= 0.724  Validation Accuracy = 0.709\n",
      "EPOCH 8 ... Training Accuracy= 0.782  Validation Accuracy = 0.758\n",
      "EPOCH 9 ... Training Accuracy= 0.823  Validation Accuracy = 0.801\n",
      "EPOCH 10 ... Training Accuracy= 0.855  Validation Accuracy = 0.830\n",
      "EPOCH 11 ... Training Accuracy= 0.879  Validation Accuracy = 0.855\n",
      "EPOCH 12 ... Training Accuracy= 0.897  Validation Accuracy = 0.873\n",
      "EPOCH 13 ... Training Accuracy= 0.911  Validation Accuracy = 0.884\n",
      "EPOCH 14 ... Training Accuracy= 0.923  Validation Accuracy = 0.896\n",
      "EPOCH 15 ... Training Accuracy= 0.928  Validation Accuracy = 0.900\n",
      "EPOCH 16 ... Training Accuracy= 0.941  Validation Accuracy = 0.914\n",
      "EPOCH 17 ... Training Accuracy= 0.948  Validation Accuracy = 0.919\n",
      "EPOCH 18 ... Training Accuracy= 0.956  Validation Accuracy = 0.928\n",
      "EPOCH 19 ... Training Accuracy= 0.959  Validation Accuracy = 0.929\n",
      "EPOCH 20 ... Training Accuracy= 0.961  Validation Accuracy = 0.933\n",
      "EPOCH 21 ... Training Accuracy= 0.963  Validation Accuracy = 0.936\n",
      "EPOCH 22 ... Training Accuracy= 0.965  Validation Accuracy = 0.936\n",
      "EPOCH 23 ... Training Accuracy= 0.972  Validation Accuracy = 0.944\n",
      "EPOCH 24 ... Training Accuracy= 0.975  Validation Accuracy = 0.947\n",
      "EPOCH 25 ... Training Accuracy= 0.977  Validation Accuracy = 0.950\n",
      "EPOCH 26 ... Training Accuracy= 0.981  Validation Accuracy = 0.953\n",
      "EPOCH 27 ... Training Accuracy= 0.980  Validation Accuracy = 0.953\n",
      "EPOCH 28 ... Training Accuracy= 0.981  Validation Accuracy = 0.954\n",
      "EPOCH 29 ... Training Accuracy= 0.983  Validation Accuracy = 0.957\n",
      "EPOCH 30 ... Training Accuracy= 0.985  Validation Accuracy = 0.958\n",
      "EPOCH 31 ... Training Accuracy= 0.986  Validation Accuracy = 0.960\n",
      "EPOCH 32 ... Training Accuracy= 0.988  Validation Accuracy = 0.961\n",
      "EPOCH 33 ... Training Accuracy= 0.987  Validation Accuracy = 0.961\n",
      "EPOCH 34 ... Training Accuracy= 0.989  Validation Accuracy = 0.965\n",
      "EPOCH 35 ... Training Accuracy= 0.988  Validation Accuracy = 0.964\n",
      "EPOCH 36 ... Training Accuracy= 0.991  Validation Accuracy = 0.964\n",
      "EPOCH 37 ... Training Accuracy= 0.991  Validation Accuracy = 0.966\n",
      "EPOCH 38 ... Training Accuracy= 0.992  Validation Accuracy = 0.968\n",
      "EPOCH 39 ... Training Accuracy= 0.993  Validation Accuracy = 0.970\n",
      "EPOCH 40 ... Training Accuracy= 0.993  Validation Accuracy = 0.969\n",
      "EPOCH 41 ... Training Accuracy= 0.992  Validation Accuracy = 0.966\n",
      "EPOCH 42 ... Training Accuracy= 0.994  Validation Accuracy = 0.970\n",
      "EPOCH 43 ... Training Accuracy= 0.994  Validation Accuracy = 0.970\n",
      "EPOCH 44 ... Training Accuracy= 0.995  Validation Accuracy = 0.972\n",
      "EPOCH 45 ... Training Accuracy= 0.994  Validation Accuracy = 0.972\n",
      "EPOCH 46 ... Training Accuracy= 0.995  Validation Accuracy = 0.972\n",
      "EPOCH 47 ... Training Accuracy= 0.995  Validation Accuracy = 0.970\n",
      "EPOCH 48 ... Training Accuracy= 0.996  Validation Accuracy = 0.974\n",
      "EPOCH 49 ... Training Accuracy= 0.996  Validation Accuracy = 0.973\n",
      "EPOCH 50 ... Training Accuracy= 0.995  Validation Accuracy = 0.970\n",
      "EPOCH 51 ... Training Accuracy= 0.997  Validation Accuracy = 0.972\n",
      "EPOCH 52 ... Training Accuracy= 0.997  Validation Accuracy = 0.974\n",
      "EPOCH 53 ... Training Accuracy= 0.997  Validation Accuracy = 0.975\n",
      "EPOCH 54 ... Training Accuracy= 0.997  Validation Accuracy = 0.975\n",
      "EPOCH 55 ... Training Accuracy= 0.997  Validation Accuracy = 0.974\n",
      "EPOCH 56 ... Training Accuracy= 0.998  Validation Accuracy = 0.976\n",
      "EPOCH 57 ... Training Accuracy= 0.997  Validation Accuracy = 0.975\n",
      "EPOCH 58 ... Training Accuracy= 0.998  Validation Accuracy = 0.977\n",
      "EPOCH 59 ... Training Accuracy= 0.998  Validation Accuracy = 0.976\n",
      "EPOCH 60 ... Training Accuracy= 0.998  Validation Accuracy = 0.975\n",
      "EPOCH 61 ... Training Accuracy= 0.998  Validation Accuracy = 0.977\n",
      "EPOCH 62 ... Training Accuracy= 0.998  Validation Accuracy = 0.977\n",
      "EPOCH 63 ... Training Accuracy= 0.999  Validation Accuracy = 0.979\n",
      "EPOCH 64 ... Training Accuracy= 0.999  Validation Accuracy = 0.977\n",
      "EPOCH 65 ... Training Accuracy= 0.998  Validation Accuracy = 0.977\n",
      "EPOCH 66 ... Training Accuracy= 0.999  Validation Accuracy = 0.977\n",
      "EPOCH 67 ... Training Accuracy= 0.998  Validation Accuracy = 0.974\n",
      "EPOCH 68 ... Training Accuracy= 0.999  Validation Accuracy = 0.979\n",
      "EPOCH 69 ... Training Accuracy= 0.999  Validation Accuracy = 0.977\n",
      "EPOCH 70 ... Training Accuracy= 0.998  Validation Accuracy = 0.976\n",
      "EPOCH 71 ... Training Accuracy= 0.999  Validation Accuracy = 0.977\n",
      "EPOCH 72 ... Training Accuracy= 0.999  Validation Accuracy = 0.979\n",
      "EPOCH 73 ... Training Accuracy= 0.999  Validation Accuracy = 0.977\n",
      "EPOCH 74 ... Training Accuracy= 0.999  Validation Accuracy = 0.977\n",
      "EPOCH 75 ... Training Accuracy= 0.999  Validation Accuracy = 0.978\n",
      "EPOCH 76 ... Training Accuracy= 0.999  Validation Accuracy = 0.979\n",
      "EPOCH 77 ... Training Accuracy= 0.999  Validation Accuracy = 0.979\n",
      "EPOCH 78 ... Training Accuracy= 0.999  Validation Accuracy = 0.978\n",
      "EPOCH 79 ... Training Accuracy= 0.999  Validation Accuracy = 0.979\n",
      "EPOCH 80 ... Training Accuracy= 0.999  Validation Accuracy = 0.979\n",
      "EPOCH 81 ... Training Accuracy= 0.999  Validation Accuracy = 0.980\n",
      "EPOCH 82 ... Training Accuracy= 0.999  Validation Accuracy = 0.978\n",
      "EPOCH 83 ... Training Accuracy= 1.000  Validation Accuracy = 0.979\n",
      "EPOCH 84 ... Training Accuracy= 0.999  Validation Accuracy = 0.978\n",
      "EPOCH 85 ... Training Accuracy= 0.999  Validation Accuracy = 0.977\n",
      "EPOCH 86 ... Training Accuracy= 0.999  Validation Accuracy = 0.979\n",
      "EPOCH 87 ... Training Accuracy= 1.000  Validation Accuracy = 0.979\n",
      "EPOCH 88 ... Training Accuracy= 0.999  Validation Accuracy = 0.980\n",
      "EPOCH 89 ... Training Accuracy= 0.999  Validation Accuracy = 0.981\n",
      "EPOCH 90 ... Training Accuracy= 1.000  Validation Accuracy = 0.980\n",
      "EPOCH 91 ... Training Accuracy= 0.999  Validation Accuracy = 0.978\n",
      "EPOCH 92 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 93 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 94 ... Training Accuracy= 0.999  Validation Accuracy = 0.981\n",
      "EPOCH 95 ... Training Accuracy= 0.999  Validation Accuracy = 0.981\n",
      "EPOCH 96 ... Training Accuracy= 0.999  Validation Accuracy = 0.982\n",
      "EPOCH 97 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 98 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 99 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 100 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 101 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 102 ... Training Accuracy= 1.000  Validation Accuracy = 0.980\n",
      "EPOCH 103 ... Training Accuracy= 1.000  Validation Accuracy = 0.980\n",
      "EPOCH 104 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 105 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 106 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 107 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 108 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 109 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 110 ... Training Accuracy= 1.000  Validation Accuracy = 0.980\n",
      "EPOCH 111 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 112 ... Training Accuracy= 0.999  Validation Accuracy = 0.978\n",
      "EPOCH 113 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 114 ... Training Accuracy= 1.000  Validation Accuracy = 0.984\n",
      "EPOCH 115 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 116 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 117 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 118 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 119 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 120 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 121 ... Training Accuracy= 1.000  Validation Accuracy = 0.980\n",
      "EPOCH 122 ... Training Accuracy= 1.000  Validation Accuracy = 0.979\n",
      "EPOCH 123 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 124 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 125 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 126 ... Training Accuracy= 1.000  Validation Accuracy = 0.980\n",
      "EPOCH 127 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 128 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 129 ... Training Accuracy= 1.000  Validation Accuracy = 0.980\n",
      "EPOCH 130 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 131 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 132 ... Training Accuracy= 1.000  Validation Accuracy = 0.984\n",
      "EPOCH 133 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 134 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 135 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 136 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 137 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 138 ... Training Accuracy= 1.000  Validation Accuracy = 0.984\n",
      "EPOCH 139 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 140 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 141 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 142 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 143 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 144 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 145 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 146 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 147 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 148 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 149 ... Training Accuracy= 1.000  Validation Accuracy = 0.981\n",
      "EPOCH 150 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 151 ... Training Accuracy= 1.000  Validation Accuracy = 0.984\n",
      "EPOCH 152 ... Training Accuracy= 1.000  Validation Accuracy = 0.985\n",
      "EPOCH 153 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 154 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 155 ... Training Accuracy= 1.000  Validation Accuracy = 0.984\n",
      "EPOCH 156 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 157 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n",
      "EPOCH 158 ... Training Accuracy= 1.000  Validation Accuracy = 0.982\n",
      "EPOCH 159 ... Training Accuracy= 1.000  Validation Accuracy = 0.984\n",
      "EPOCH 160 ... Training Accuracy= 1.000  Validation Accuracy = 0.983\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test_load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa1147dd39fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m#file_writer.add_summary(summary,i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mTest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mTest_load_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_load' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "\n",
    "def LeNet(x,keep_prob, fcp=(120,120,84)):\n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "\n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean=mu, stddev=sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    with tf.name_scope(\"RELU1\"):\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    with tf.name_scope(\"POOL1\"):\n",
    "        conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    with tf.name_scope(\"CONV2\"):\n",
    "        conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n",
    "        conv2_b = tf.Variable(tf.zeros(16))\n",
    "        conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    with tf.name_scope(\"RELU2\"):\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    with tf.name_scope(\"POOL2\"):\n",
    "        # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "        conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    with tf.name_scope(\"Flatten\"):\n",
    "        # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "        fc0 = flatten(conv2)\n",
    "\n",
    "    with tf.name_scope(\"FC1\"):\n",
    "        # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "        fc1_W = tf.Variable(tf.truncated_normal(shape=(400, fcp[0]), mean=mu, stddev=sigma))\n",
    "        fc1_b = tf.Variable(tf.zeros(fcp[0]))\n",
    "        fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "\n",
    "    with tf.name_scope(\"RELU3\"):\n",
    "        # SOLUTION: Activation.\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    with tf.name_scope(\"DROP_OUT\"):\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob=keep_prob)\n",
    "\n",
    "    with tf.name_scope(\"FC2\"):\n",
    "        # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        fc2_W = tf.Variable(tf.truncated_normal(shape=(fcp[0], fcp[1]), mean=mu, stddev=sigma))\n",
    "        fc2_b = tf.Variable(tf.zeros(fcp[1]))\n",
    "        fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "\n",
    "    with tf.name_scope(\"RELU4\"):\n",
    "        # SOLUTION: Activation.\n",
    "        fc2 = tf.nn.relu(fc2)\n",
    "        \n",
    "        \n",
    "    with tf.name_scope(\"FC2\"):\n",
    "        # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        fc3_W = tf.Variable(tf.truncated_normal(shape=(fcp[1], fcp[2]), mean=mu, stddev=sigma))\n",
    "        fc3_b = tf.Variable(tf.zeros(fcp[2]))\n",
    "        fc3 = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "\n",
    "    with tf.name_scope(\"RELU5\"):\n",
    "        # SOLUTION: Activation.\n",
    "        fc3 = tf.nn.relu(fc3)\n",
    "        \n",
    "\n",
    "    with tf.name_scope(\"FC4\"):\n",
    "        # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "        fc4_W = tf.Variable(tf.truncated_normal(shape=(fcp[2], 43), mean=mu, stddev=sigma))\n",
    "        fc4_b = tf.Variable(tf.zeros(43))\n",
    "        logits = tf.matmul(fc3, fc4_W) + fc4_b\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32, ())\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 160\n",
    "BATCH_SIZE = 1024\n",
    "rate = 0.001\n",
    "training_keep_prob = 0.5\n",
    "\n",
    "netpara = (300,180,100)\n",
    "#netpara = (120, 84)\n",
    "\n",
    "logits = LeNet(x,keep_prob, netpara)\n",
    "#logits = LeNet_o(x,keep_prob, netpara)\n",
    "\n",
    "tf.summary.image(\"x\", x,max_outputs=3)\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy_operation)\n",
    "#tf.summary.scalar(\"validation_accuracy\", validation_accuracy)\n",
    "\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob:1})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # sess.graph_def is the graph definition; that enables the Graph Visualizer.\n",
    "    file_writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob:training_keep_prob})\n",
    "\n",
    "            if 0: #offset % (BATCH_SIZE*100) == 0.0:\n",
    "                summary, Traing_loss = sess.run([merged,loss_operation], feed_dict={x: batch_x, y: batch_y, keep_prob:1})\n",
    "                print(\"batch = {:.3f}  Traing Loss = {:.3f}\".format(offset, Traing_loss))\n",
    "\n",
    "                file_writer.add_summary(summary, i*num_examples+offset)\n",
    "\n",
    "\n",
    "        Training_accuracy = evaluate(X_train, y_train)\n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ... Training Accuracy= {:.3f}  Validation Accuracy = {:.3f}\".format(i+1, Training_accuracy,validation_accuracy))\n",
    "\n",
    "        #file_writer.add_summary(summary,i)\n",
    "    Test_accuracy = evaluate(X_test, y_test)\n",
    "    Test_load_accuracy = evaluate(X_test_load, y_test_load)\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Test Accuracy= {:.3f}  Test_load Accuracy = {:.3f}\".format(Test_accuracy,Test_load_accuracy))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web. \n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the IPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Project Writeup\n",
    "\n",
    "Once you have completed the code implementation, document your results in a project writeup using this [template](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup_template.md) as a guide. The writeup can be in a markdown or pdf file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "336px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
